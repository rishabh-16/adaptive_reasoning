{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the Qwen3-30B-A3B model from the specified cache directory\n",
    "model_name = \"Qwen/Qwen3-30B-A3B\"\n",
    "cache_dir = \"/fsx-project/rishabhtiwari/hf_cache\"\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(f\"Cache directory: {cache_dir}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_dir,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load model\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# Load config first\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_dir,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Initialize model from config without pretrained weights\n",
    "model = AutoModelForCausalLM.from_config(\n",
    "    config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "# model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "config_path = hf_hub_download(repo_id=\"open-thoughts/OpenThoughts3-1.2M\", filename=\"data\", repo_type=\"dataset\", cache_dir=\"/fsx-project/rishabhtiwari/hf_cache\")\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"open-thoughts/OpenThoughts3-1.2M\", cache_dir=\"/fsx-project/rishabhtiwari/hf_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_dataset(\"open-thoughts/OpenThoughts3-1.2M\", cache_dir=\"/fsx-project/rishabhtiwari/hf_cache\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset based on source\n",
    "# First, let's see what sources are available\n",
    "domains = set(dataset['train']['domain'])\n",
    "print(\"Available domains:\")\n",
    "for domain in sorted(domains):\n",
    "    print(f\"  - {domain}\")\n",
    "\n",
    "print(f\"\\nTotal number of domains: {len(domains)}\")\n",
    "\n",
    "# Example: Filter by a specific source (replace 'your_source' with actual source name)\n",
    "# filtered_dataset = dataset['train'].filter(lambda x: x['source'] == 'your_source')\n",
    "\n",
    "# Example: Filter by multiple sources\n",
    "domains_to_keep = ['math']\n",
    "filtered_dataset = dataset['train'].filter(lambda x: x['domain'] in domains_to_keep)\n",
    "\n",
    "# Example: Filter out specific sources\n",
    "# sources_to_exclude = ['source_to_exclude']\n",
    "# filtered_dataset = dataset['train'].filter(lambda x: x['source'] not in sources_to_exclude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'] = filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered dataset\n",
    "dataset.save_to_disk(\"/fsx-project/rishabhtiwari/datasets/openthoughts_math_filtered\")\n",
    "print(\"Dataset saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a few datapoints from the dataset\n",
    "for i in range(3):\n",
    "    print(f\"=== Datapoint {i+1} ===\")\n",
    "    example = filtered_dataset[i]\n",
    "    print(f\"Difficulty: {example['difficulty']}\")\n",
    "    print(f\"Source: {example['source']}\")\n",
    "    print(f\"Domain: {example['domain']}\")\n",
    "    print(\"Conversations:\")\n",
    "    for j, conv in enumerate(example['conversations']):\n",
    "        print(f\"  {j+1}. From: {conv['from']}\")\n",
    "        print(f\"     Value: {conv['value'][:200]}{'...' if len(conv['value']) > 200 else ''}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
