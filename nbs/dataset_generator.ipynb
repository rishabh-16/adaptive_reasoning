{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d98b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_PATH = \"/home/rishabhtiwari/hf_cache/Qwen--Qwen3-30B-A3B\"\n",
    "DATASET_NAME = \"open-thoughts/OpenThoughts-3\" \n",
    "LOCAL_DATASET_PATH = \"/home/rishabhtiwari/datasets/openthoughts3_small\"\n",
    "OUTPUT_PATH = \"/home/rishabhtiwari/datasets/openthoughts3_reannotated\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c2da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vLLM\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "\n",
    "tensor_parallel_size = torch.cuda.device_count()\n",
    "print(f\"Using {tensor_parallel_size} GPUs for tensor parallelism.\")\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    tensor_parallel_size=tensor_parallel_size,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"bfloat16\",\n",
    "    gpu_memory_utilization=0.90\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_tokens=2048\n",
    ")\n",
    "\n",
    "# Get tokenizer from vLLM to ensure consistency\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2165fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "try:\n",
    "    print(f\"Attempting to load dataset: {DATASET_NAME}\")\n",
    "    dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load from Hugging Face: {e}\")\n",
    "    if os.path.exists(LOCAL_DATASET_PATH):\n",
    "        print(f\"Loading from local path: {LOCAL_DATASET_PATH}\")\n",
    "        dataset = load_from_disk(LOCAL_DATASET_PATH)\n",
    "    else:\n",
    "        print(\"Dataset not found. Creating dummy dataset for testing.\")\n",
    "        dataset = Dataset.from_list([\n",
    "            {\"conversations\": [{\"role\": \"user\", \"content\": \"Solve 2x + 5 = 15\"}, {\"role\": \"assistant\", \"content\": \"Old answer\"}]},\n",
    "            {\"conversations\": [{\"role\": \"user\", \"content\": \"Explain quantum entanglement\"}, {\"role\": \"assistant\", \"content\": \"Old answer\"}]}\n",
    "        ])\n",
    "\n",
    "print(f\"Loaded dataset with {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare prompts\n",
    "prompts = []\n",
    "indices_to_process = []\n",
    "original_examples = []\n",
    "\n",
    "print(\"Preparing prompts...\")\n",
    "for idx, example in enumerate(tqdm(dataset)):\n",
    "    instruction = None\n",
    "    if \"conversations\" in example:\n",
    "        for msg in example[\"conversations\"]:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                instruction = msg[\"content\"]\n",
    "                break\n",
    "    elif \"instruction\" in example:\n",
    "        instruction = example[\"instruction\"]\n",
    "    \n",
    "    if instruction:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": instruction}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "        indices_to_process.append(idx)\n",
    "        original_examples.append(example)\n",
    "\n",
    "print(f\"Prepared {len(prompts)} prompts for generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64261d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "print(\"Starting generation with vLLM...\")\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Process results\n",
    "new_data = []\n",
    "\n",
    "# Map results back to examples\n",
    "for i, output in enumerate(outputs):\n",
    "    generated_text = output.outputs[0].text\n",
    "    example = original_examples[i]\n",
    "    \n",
    "    if \"conversations\" in example:\n",
    "        new_convs = []\n",
    "        for msg in example[\"conversations\"]:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                new_convs.append(msg)\n",
    "                new_convs.append({\"role\": \"assistant\", \"content\": generated_text})\n",
    "                break\n",
    "        example[\"conversations\"] = new_convs\n",
    "    elif \"output\" in example:\n",
    "        example[\"output\"] = generated_text\n",
    "    \n",
    "    new_data.append(example)\n",
    "\n",
    "# Add any examples that weren't processed (e.g. no instruction found) if needed\n",
    "# In this logic we only keep processed ones. To keep all, we'd need to merge with indices not in indices_to_process.\n",
    "# For simplicity, we'll assume we want to keep all and just copy the unprocessed ones.\n",
    "processed_indices = set(indices_to_process)\n",
    "for idx, example in enumerate(dataset):\n",
    "    if idx not in processed_indices:\n",
    "        new_data.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d974558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset and save\n",
    "final_dataset = Dataset.from_list(new_data)\n",
    "final_dataset.save_to_disk(OUTPUT_PATH)\n",
    "print(f\"Finished! Saved {len(final_dataset)} examples to {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
